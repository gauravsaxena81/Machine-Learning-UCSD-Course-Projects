\relax 
\citation{efron1975efficiency}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Logistic Regression}{\thepage }}
\citation{malouf2002comparison}
\citation{bro2003centering}
\citation{babyak2004you}
\citation{ng2004feature}
\citation{bro2003centering}
\newlabel{eq:max}{{2}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Gradient Descent}{\thepage }}
\newlabel{eq:betaUpdate}{{3}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Feature Scaling}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Over-Fitting}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {3}Implementation}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}SGD}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Normalization of training data}{\thepage }}
\citation{settingHyperparameters}
\citation{lbfgsMatlabImpl}
\@writefile{toc}{\contentsline {paragraph}{Method 1}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Method 2}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Tuning the initial guess}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}L-BFGS}{\thepage }}
\newlabel{SGD_(not_Regularized)_Accuracy_vs_Epochs}{{4}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Stochastic Gradient Descent: Accuracy of trained models improves with number of training epochs}}{\thepage }}
\newlabel{accuracy_vs_epochs_l2_regularized}{{4}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces $L_{2}$ Regularized Stochastic Gradient Descent : Accuracy of trained models improves with number of training epochs}}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{\thepage }}
\newlabel{SGD_Regularized_Log_Likelihood}{{4}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces $L_{2}$ Regularized Stochastic Gradient Descent :Regularized LCL vs Number of Epochs}}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Stochastic Gradient Descent : LCL vs Number of Epochs}}{\thepage }}
\newlabel{accuracy_vs_mu_l2_regularized}{{4}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces $L_{2}$ Regularized Stochastic Gradient Descent : Accuracy vs Hyperparameter $\mu $}}{\thepage }}
\newlabel{LBFGS_Graph}{{4.1}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces L-BFGS : Accuracy vs parameter $log_2{\mu }$}}{\thepage }}
\newlabel{All_accuracies_in_one}{{4.1}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Accuracies for all SGD variants - $L_{2}$ Regularized and vanilla SGD}}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}L-BFGS}{\thepage }}
\citation{mpComp}
\bibstyle{abbrv}
\bibdata{Report}
\bibcite{mpComp}{1}
\bibcite{babyak2004you}{2}
\bibcite{settingHyperparameters}{3}
\bibcite{bro2003centering}{4}
\bibcite{efron1975efficiency}{5}
\bibcite{malouf2002comparison}{6}
\bibcite{ng2004feature}{7}
\bibcite{lbfgsMatlabImpl}{8}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Accuracy}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {6}References}{\thepage }}
